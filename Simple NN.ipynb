{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, derivative=False):\n",
    "    if(derivative==True):\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid function normally returns the exponential expression, but for simplicity we have also added the derivative of the sigmoid fuction in the same snippet, as the derivative will be required each time we try to back propagate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x =np.array([[0,0,1,0],\n",
    "            [0,1,1,1],\n",
    "            [1,0,1,1],\n",
    "            [1,1,1,0],\n",
    "             [1,1,1,1]\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([[0],\n",
    "              [1],\n",
    "              [1],\n",
    "              [0],\n",
    "              [1]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1) #so that the randomization follows the same pattern each time owing to only one seed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "synp0 = 2*np.random.random((4,5))-1 \n",
    "synp1 = 2*np.random.random((5,1))-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the two syanpses' shapes. It has been created in such a way that the end dot product or the output of the whole model if always have the same dimensions as the output we need ie. 5x4 and 4x5 gives 5x5 which when multiplied by 5x1 will give me the desired Matrix dimensions. Keep in mind when creating the synapses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4714637058546244\n",
      "0.0025969240948557674\n",
      "0.0018708228699543575\n",
      "0.0015432160663121548\n",
      "0.001345435918701168\n",
      "0.0012092042715092205\n",
      "0.001107917462877487\n",
      "0.0010287423298040769\n",
      "0.0009646166520256849\n",
      "0.0009112867106074955\n"
     ]
    }
   ],
   "source": [
    "for j in range(100000):\n",
    "    l0 =x\n",
    "    l1 = sigmoid(np.dot(l0, synp0)) #Layer wise feeding. \n",
    "    l2 = sigmoid(np.dot(l1,synp1))\n",
    "    l2_error = y-l2 #gives me the error matrix for our output. \n",
    "    \n",
    "    \n",
    "    if( j%10000) ==0:\n",
    "        print(np.mean(np.abs(l2_error)))\n",
    "    l2_delta = l2_error*sigmoid(l2, derivative = True) #derivative according to the definition of back propagation. \n",
    "    #(5x1)\n",
    "    l1_error = l2_error.dot(synp1.T) #calculating the Error of each layer and getting the derivative. Since our Error in layer 2 is directly due to the weights assigned in Synapses 1, we can correlate them. \n",
    "    #(5x5)\n",
    "    l1_delta = l1_error*sigmoid(l1,derivative=True) #the Derivative is to be fed to the Synapses to correct their values. \n",
    "    #(5x5)\n",
    "    synp1 +=l1.T.dot(l2_delta)  #(5x1)\n",
    "    synp0 +=l0.T.dot(l1_delta)  #(4x5)\n",
    "    #notice how the choice of indices are such that we always get the one we started with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00107008]\n",
      " [0.99926779]\n",
      " [0.99926777]\n",
      " [0.0010614 ]\n",
      " [0.99926583]]\n"
     ]
    }
   ],
   "source": [
    "print(l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the values are approximating to our given Output. Changing the number of times the loop is to run will no longer give a definite and stark change in precision. Adding another layer to our model might help in getting a noticable difference in precision. \n",
    "The things to Keep in mind:\n",
    "1. No matter how many layers, you have to adjust the syanpses indicies to match with our output and also such that matrix multiplication is possible. \n",
    "2. Always keep a close eye on how your matrix index changes after each operation because things can get ugly and hard to track especially when back propagating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
